{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I can do all this through him who givs me strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle : New York City Taxi Trip Duration\n",
    "### Share code and data to improve ride time predictions\n",
    "\n",
    "https://www.kaggle.com/c/nyc-taxi-trip-duration/kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jk/.local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Map\n",
    "import folium\n",
    "import folium.plugins as plugins\n",
    "from folium.plugins import MarkerCluster   \n",
    "from geographiclib.geodesic import Geodesic\n",
    "\n",
    "#distance\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import haversine\n",
    "\n",
    "# time/day data\n",
    "import time, datetime                        \n",
    "from collections import namedtuple\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "\n",
    "# Regression\n",
    "import scipy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "# Decission Tree regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Understanding Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Datetime 다루기\n",
    "\n",
    "datetime은 pd.to_datetime method(?)를 사용해야 datetime에서 년, 월, 일 Data를 활용할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickup/Dropoff_Datetime Datatype 변경\n",
    "\n",
    "train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'])\n",
    "train['dropoff_datetime'] = pd.to_datetime(train['dropoff_datetime'])\n",
    "\n",
    "test['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#day of week\n",
    "#date by month\n",
    "train[\"pick_month\"] = train[\"pickup_datetime\"].dt.month.astype(\"int\")\n",
    "test[\"pick_month\"] = test[\"pickup_datetime\"].dt.month.astype(\"int\")\n",
    "\n",
    "#Monday=0, Sunday=6\n",
    "train[\"pick_week\"] = train[\"pickup_datetime\"].dt.dayofweek.astype(\"int\")\n",
    "test[\"pick_week\"] = test[\"pickup_datetime\"].dt.dayofweek.astype(\"int\")\n",
    "\n",
    "#date\n",
    "train['pick_date'] = train['pickup_datetime'].dt.date\n",
    "test['pick_date'] = test['pickup_datetime'].dt.date\n",
    "\n",
    "#date by hour\n",
    "train[\"pick_hour\"] = train[\"pickup_datetime\"].dt.hour.astype(\"int\")\n",
    "test[\"pick_hour\"] = test[\"pickup_datetime\"].dt.hour.astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Time division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 8, 18, 24]\n",
    "group_names = [\"before work\", \"work\", \"after work\"]\n",
    "train['work_hour'] = pd.cut(train['pick_hour'], bins, labels=group_names, right=False)\n",
    "\n",
    "test['work_hour'] = pd.cut(test['pick_hour'], bins, labels=group_names, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a list of holidays in the US\n",
    "calendar = USFederalHolidayCalendar()\n",
    "holidays = calendar.holidays()\n",
    "\n",
    "# Load business days\n",
    "us_bd = CustomBusinessDay(calendar = USFederalHolidayCalendar())\n",
    "# Set business_days equal to the work days in our date range.\n",
    "business_days = pd.DatetimeIndex(start = train.pickup_datetime.min(), \n",
    "                                 end = train.pickup_datetime.max(), \n",
    "                                 freq = us_bd)\n",
    "business_days = pd.to_datetime(business_days).date\n",
    "# Fot test\n",
    "business_days = pd.DatetimeIndex(start = test.pickup_datetime.min(), \n",
    "                                 end = test.pickup_datetime.max(), \n",
    "                                 freq = us_bd)\n",
    "business_days = pd.to_datetime(business_days).date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features relating to time\n",
    "train['pickup_holiday'] = pd.to_datetime(train.pickup_datetime.dt.date).isin(holidays)\n",
    "train['pickup_holiday'] = train.pickup_holiday.map(lambda x: 1 if x == True else 0)\n",
    "\n",
    "train['pickup_businessday'] = pd.to_datetime(train.pickup_datetime.dt.date).isin(business_days)\n",
    "train['pickup_businessday'] = train.pickup_businessday.map(lambda x: 1 if x == True else 0)\n",
    "\n",
    "\n",
    "test['pickup_holiday'] = pd.to_datetime(test.pickup_datetime.dt.date).isin(holidays)\n",
    "test['pickup_holiday'] = test.pickup_holiday.map(lambda x: 1 if x == True else 0)\n",
    "\n",
    "test['pickup_businessday'] = pd.to_datetime(test.pickup_datetime.dt.date).isin(business_days)\n",
    "test['pickup_businessday'] = test.pickup_businessday.map(lambda x: 1 if x == True else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont' need this feature any more\n",
    "train = train.drop('pickup_datetime', 1)\n",
    "test = test.drop('pickup_datetime', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Haversine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_long_border = (-74.03, -73.75)\n",
    "city_lat_border = (40.63, 40.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['haversine_distance'] = train.apply(lambda r: haversine.haversine((r['pickup_latitude'],r['pickup_longitude']),\n",
    "                                                                  (r['dropoff_latitude'], r['dropoff_longitude'])), axis=1)\n",
    "\n",
    "test['haversine_distance'] = test.apply(lambda r: haversine.haversine((r['pickup_latitude'],r['pickup_longitude']),\n",
    "                                                                  (r['dropoff_latitude'], r['dropoff_longitude'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = 6371.0\n",
    "\n",
    "# dist = []\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     lat1 = radians(taxi1.iloc[i,6])\n",
    "#     lon1 = radians(taxi1.iloc[i,5])\n",
    "#     lat2 = radians(taxi1.iloc[i,8])\n",
    "#     lon2 = radians(taxi1.iloc[i,7])\n",
    "\n",
    "#     dlon = lon2 - lon1\n",
    "#     dlat = lat2 - lat1\n",
    "\n",
    "#     a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "#     c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "#     distance = R * c\n",
    "#     dist.append(distance)\n",
    "    \n",
    "# df['distance'] = dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Manhattan Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['manhattan_distance'] = (abs(train.dropoff_longitude - train.pickup_longitude) +\n",
    "                            abs(train.dropoff_latitude - train.pickup_latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['manhattan_distance'] = (abs(test.dropoff_longitude - test.pickup_longitude) +\n",
    "                            abs(test.dropoff_latitude - test.pickup_latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bearing(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n",
    "    '''Calculate the direction of travel in degrees'''\n",
    "    pickup_lat_rads = np.radians(pickup_lat)\n",
    "    pickup_long_rads = np.radians(pickup_long)\n",
    "    dropoff_lat_rads = np.radians(dropoff_lat)\n",
    "    dropoff_long_rads = np.radians(dropoff_long)\n",
    "    long_delta_rads = np.radians(dropoff_long_rads - pickup_long_rads)\n",
    "    \n",
    "    y = np.sin(long_delta_rads) * np.cos(dropoff_lat_rads)\n",
    "    x = (np.cos(pickup_lat_rads) * \n",
    "         np.sin(dropoff_lat_rads) - \n",
    "         np.sin(pickup_lat_rads) * \n",
    "         np.cos(dropoff_lat_rads) * \n",
    "         np.cos(long_delta_rads))\n",
    "    \n",
    "    return np.degrees(np.arctan2(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bearing'] = calculate_bearing(train.pickup_latitude,\n",
    "                                     train.pickup_longitude,\n",
    "                                     train.dropoff_latitude,\n",
    "                                     train.dropoff_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['bearing'] = calculate_bearing(test.pickup_latitude,\n",
    "                                     test.pickup_longitude,\n",
    "                                     test.dropoff_latitude,\n",
    "                                     test.dropoff_longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Removing Outliner based on rational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train.haversine_distance > 200] = np.nan ##200km 넘는 데이터 제거\n",
    "train.loc[train.trip_duration > 40000] = np.nan ##40000초(약 11시간)가 넘는 데이터 제거\n",
    "train.loc[train.passenger_count == 0] = np.NAN   ### passenger 수가 0인 데이터 제거\n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1456576, 20)\n",
      "(625134, 18)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458639    6.658011\n",
       "1458640    6.486161\n",
       "1458641    6.639876\n",
       "1458642    5.924256\n",
       "1458643    5.293305\n",
       "Name: log_duration, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['log_duration'] = np.log1p(train['trip_duration'])\n",
    "train.loc[:, 'log_duration'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126c77ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train['log_duration'].values, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Average Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:, 'avg_speed_h'] = 1000 * train['haversine_distance'] / train['trip_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Average Speed for regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"p_lon\"] = np.round(train['pickup_longitude'], 2)\n",
    "# train[\"p_lat\"] = np.round(train['pickup_latitude'], 2)\n",
    "# train[\"d_lon\"] = np.round(train['dropoff_longitude'], 2)\n",
    "# train[\"d_lat\"] = np.round(train['dropoff_latitude'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_p = train[['avg_speed_h', 'p_lon', 'p_lat']].groupby(['p_lon', 'p_lat']).mean().reset_index()\n",
    "# train_p = train.rename(columns = {'avg_speed_h': 'ave_speed_p'})\n",
    "# train_d = train[['avg_speed_h', 'd_lon', 'd_lat']].groupby(['d_lon', 'd_lat']).mean().reset_index()\n",
    "# train_d = train_d.rename(columns = {'avg_speed_h': 'ave_speed_d'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.merge(train_p, train_d, on = ['d_lon', 'd_lat'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[\"p_lon\"] = np.round(test['pickup_longitude'], 2)\n",
    "# test[\"p_lat\"] = np.round(test['pickup_latitude'], 2)\n",
    "# test[\"d_lon\"] = np.round(test['dropoff_longitude'], 2)\n",
    "# test[\"d_lat\"] = np.round(test['dropoff_latitude'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_p.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Coordinate of lat, long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords = np.vstack((taxi1[['pickup_latitude', 'pickup_longitude']].values,\n",
    "#                     taxi1[['dropoff_latitude', 'dropoff_longitude']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_ind = np.random.permutation(len(coords))[:500000]\n",
    "# kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxi1.loc[:, 'pickup_cluster'] = kmeans.predict(taxi1[['pickup_latitude', 'pickup_longitude']])\n",
    "# taxi1.loc[:, 'dropoff_cluster'] = kmeans.predict(taxi1[['dropoff_latitude', 'dropoff_longitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Pair of Speed & Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Create dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'vendor_id', 'dropoff_datetime', 'passenger_count',\n",
      "       'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
      "       'dropoff_latitude', 'store_and_fwd_flag', 'trip_duration', 'pick_month',\n",
      "       'pick_week', 'pick_date', 'pick_hour', 'work_hour', 'pickup_holiday',\n",
      "       'pickup_businessday', 'haversine_distance', 'manhattan_distance',\n",
      "       'bearing', 'log_duration', 'avg_speed_h'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>...</th>\n",
       "      <th>pick_date</th>\n",
       "      <th>pick_hour</th>\n",
       "      <th>work_hour</th>\n",
       "      <th>pickup_holiday</th>\n",
       "      <th>pickup_businessday</th>\n",
       "      <th>haversine_distance</th>\n",
       "      <th>manhattan_distance</th>\n",
       "      <th>bearing</th>\n",
       "      <th>log_duration</th>\n",
       "      <th>avg_speed_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>17.0</td>\n",
       "      <td>work</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.498521</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>174.333195</td>\n",
       "      <td>6.122493</td>\n",
       "      <td>3.293452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-06-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>before work</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.805507</td>\n",
       "      <td>0.026478</td>\n",
       "      <td>-178.051506</td>\n",
       "      <td>6.498282</td>\n",
       "      <td>2.723239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>11.0</td>\n",
       "      <td>work</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.385098</td>\n",
       "      <td>0.080158</td>\n",
       "      <td>-179.629721</td>\n",
       "      <td>7.661527</td>\n",
       "      <td>3.006167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>19.0</td>\n",
       "      <td>after work</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.485498</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>-179.872566</td>\n",
       "      <td>6.063785</td>\n",
       "      <td>3.462700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>13.0</td>\n",
       "      <td>work</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.188588</td>\n",
       "      <td>0.010818</td>\n",
       "      <td>179.990812</td>\n",
       "      <td>6.077642</td>\n",
       "      <td>2.732387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id    dropoff_datetime  passenger_count  \\\n",
       "0  id2875421        2.0 2016-03-14 17:32:30              1.0   \n",
       "1  id2377394        1.0 2016-06-12 00:54:38              1.0   \n",
       "2  id3858529        2.0 2016-01-19 12:10:48              1.0   \n",
       "3  id3504673        2.0 2016-04-06 19:39:40              1.0   \n",
       "4  id2181028        2.0 2016-03-26 13:38:10              1.0   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "0        -73.982155        40.767937         -73.964630         40.765602   \n",
       "1        -73.980415        40.738564         -73.999481         40.731152   \n",
       "2        -73.979027        40.763939         -74.005333         40.710087   \n",
       "3        -74.010040        40.719971         -74.012268         40.706718   \n",
       "4        -73.973053        40.793209         -73.972923         40.782520   \n",
       "\n",
       "  store_and_fwd_flag  trip_duration     ...        pick_date  pick_hour  \\\n",
       "0                  N          455.0     ...       2016-03-14       17.0   \n",
       "1                  N          663.0     ...       2016-06-12        0.0   \n",
       "2                  N         2124.0     ...       2016-01-19       11.0   \n",
       "3                  N          429.0     ...       2016-04-06       19.0   \n",
       "4                  N          435.0     ...       2016-03-26       13.0   \n",
       "\n",
       "     work_hour  pickup_holiday pickup_businessday  haversine_distance  \\\n",
       "0         work             0.0                1.0            1.498521   \n",
       "1  before work             0.0                0.0            1.805507   \n",
       "2         work             0.0                1.0            6.385098   \n",
       "3   after work             0.0                1.0            1.485498   \n",
       "4         work             0.0                0.0            1.188588   \n",
       "\n",
       "   manhattan_distance     bearing  log_duration  avg_speed_h  \n",
       "0            0.019859  174.333195      6.122493     3.293452  \n",
       "1            0.026478 -178.051506      6.498282     2.723239  \n",
       "2            0.080158 -179.629721      7.661527     3.006167  \n",
       "3            0.015480 -179.872566      6.063785     3.462700  \n",
       "4            0.010818  179.990812      6.077642     2.732387  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy features for these features, then drop these features\n",
    "dummies = ['work_hour', 'pickup_holiday', 'pickup_businessday',\n",
    "          'passenger_count', 'vendor_id', 'store_and_fwd_flag']\n",
    "for feature in dummies:\n",
    "    dummy_features = pd.get_dummies(train[feature], prefix=feature)\n",
    "    for dummy in dummy_features:\n",
    "        train[dummy] = dummy_features[dummy]\n",
    "    train = train.drop([feature], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456576, 36)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1456576, 30) \n",
      "\n",
      " Index(['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
      "       'dropoff_latitude', 'pick_month', 'pick_week', 'haversine_distance',\n",
      "       'manhattan_distance', 'bearing', 'avg_speed_h', 'work_hour_before work',\n",
      "       'work_hour_work', 'work_hour_after work', 'pickup_holiday_0.0',\n",
      "       'pickup_holiday_1.0', 'pickup_businessday_0.0',\n",
      "       'pickup_businessday_1.0', 'passenger_count_1.0', 'passenger_count_2.0',\n",
      "       'passenger_count_3.0', 'passenger_count_4.0', 'passenger_count_5.0',\n",
      "       'passenger_count_6.0', 'passenger_count_7.0', 'passenger_count_8.0',\n",
      "       'passenger_count_9.0', 'vendor_id_1.0', 'vendor_id_2.0',\n",
      "       'store_and_fwd_flag_N', 'store_and_fwd_flag_Y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dfx = train.drop(['id','dropoff_datetime','pick_hour', 'pick_date', 'trip_duration','log_duration'],1)\n",
    "print(dfx.shape,'\\n\\n', dfx.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['log_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = ['work_hour', 'pickup_holiday', 'pickup_businessday',\n",
    "          'vendor_id', 'store_and_fwd_flag']\n",
    "for feature in dummies:\n",
    "    dummy_features = pd.get_dummies(test[feature], prefix=feature)\n",
    "    for dummy in dummy_features:\n",
    "        test[dummy] = dummy_features[dummy]\n",
    "    test = test.drop([feature], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx_test = test.drop(['id', 'pick_hour', 'pick_date'],1)\n",
    "print(dfx_test.shape,'\\n\\n', dfx_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Test Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dfx, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97128694 0.97070696 0.97067991 0.96940014 0.97232061]\n"
     ]
    }
   ],
   "source": [
    "model_dt=DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, \n",
    "                              min_samples_split=2, min_samples_leaf=1, \n",
    "                              min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                              random_state=0, max_leaf_nodes=None, presort=False)\n",
    "model_dt.fit(X_train, y_train)\n",
    "print(cross_val_score(model_dt,dfx,y,cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 6 (0.707880)\n",
      "2. feature 9 (0.269744)\n",
      "3. feature 27 (0.004703)\n",
      "4. feature 0 (0.003590)\n",
      "5. feature 1 (0.003077)\n",
      "6. feature 3 (0.002965)\n",
      "7. feature 2 (0.002764)\n",
      "8. feature 4 (0.001408)\n",
      "9. feature 5 (0.001208)\n",
      "10. feature 7 (0.000599)\n",
      "11. feature 12 (0.000356)\n",
      "12. feature 11 (0.000306)\n",
      "13. feature 17 (0.000301)\n",
      "14. feature 10 (0.000243)\n",
      "15. feature 18 (0.000172)\n",
      "16. feature 16 (0.000167)\n",
      "17. feature 15 (0.000146)\n",
      "18. feature 19 (0.000132)\n",
      "19. feature 21 (0.000057)\n",
      "20. feature 29 (0.000048)\n",
      "21. feature 20 (0.000040)\n",
      "22. feature 8 (0.000022)\n",
      "23. feature 28 (0.000019)\n",
      "24. feature 13 (0.000018)\n",
      "25. feature 22 (0.000018)\n",
      "26. feature 14 (0.000015)\n",
      "27. feature 23 (0.000002)\n",
      "28. feature 26 (0.000001)\n",
      "29. feature 24 (0.000000)\n",
      "30. feature 25 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "importances = model_dt.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# std = np.std([model_dt.feature_importances_ for treet in model_dt.estimators_],\n",
    "#             axis=0)\n",
    "\n",
    "\n",
    "# # Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(dfx.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# # Plot the feature importances of the forest\n",
    "# plt.figure()\n",
    "# plt.title(\"Feature importances\")\n",
    "# plt.bar(range(dfx.shape[1]), importances[indices],\n",
    "#        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "# plt.xticks(range(dfx.shape[1]), indices)\n",
    "# plt.xlim([-1, dfx.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model_dt.predict(dfx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.exp(y_test) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shpae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([test['id'], y_test], axis=1)\n",
    "submission.columns = ['id','trip_duration']\n",
    "submission['trip_duration'] = submission.apply(lambda x : 1 if (x['trip_duration'] <= 0) else x['trip_duration'], axis = 1)\n",
    "submission.to_csv(\"submission_.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest  (오래걸림)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Random Forest Regressor\n",
    "model_rnd_frst=RandomForestRegressor(n_estimators=10, criterion='mse', max_depth=None, \n",
    "                                    min_samples_split=2, min_samples_leaf=1, \n",
    "                                    min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                                    max_leaf_nodes=None, min_impurity_decrease=1e-07, \n",
    "                                    bootstrap=True, oob_score=False, n_jobs=-1, \n",
    "                                    random_state=0, verbose=1, warm_start=False)\n",
    "model_rnd_frst.fit(X_train, y_train)\n",
    "print(cross_val_score(model_rnd_frst,dfx,y,cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1 = model_rnd_frst.predict(dfx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1 = np.exp(y_test1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1 = pd.DataFrame(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1.shpae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([test['id'], y_test1], axis=1)\n",
    "submission.columns = ['id','trip_duration']\n",
    "submission['trip_duration'] = submission.apply(lambda x : 1 if (x['trip_duration'] <= 0) else x['trip_duration'], axis = 1)\n",
    "submission.to_csv(\"submission_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b6c4c3599c96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                   max_leaf_nodes=None, warm_start=False, presort='auto')\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel_gb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_gb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdfx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1034\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 788\u001b[0;31m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_gb=GradientBoostingRegressor(loss='ls', learning_rate=0.05, n_estimators=400, subsample=1.0,\n",
    "                                  criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, max_depth=5,\n",
    "                                  init=None, random_state=None, max_features=None, alpha=0.9, \n",
    "                                  verbose=0, \n",
    "                                  max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "model_gb.fit(X_train, y_train)\n",
    "print(cross_val_score(model_gb,dfx,y,cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = model_rnd_frst.predict(dfx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = np.exp(y_test2) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = pd.DataFrame(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2.shpae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([test['id'], y_test2], axis=1)\n",
    "submission.columns = ['id','trip_duration']\n",
    "submission['trip_duration'] = submission.apply(lambda x : 1 if (x['trip_duration'] <= 0) else x['trip_duration'], axis = 1)\n",
    "submission.to_csv(\"submission_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_KNN=KNeighborsRegressor(n_neighbors=5, weights='uniform', \n",
    "                                               algorithm='auto', leaf_size=30, p=2, \n",
    "                                               metric='minkowski', metric_params=None,\n",
    "                                               n_jobs=-1)\n",
    "print(cross_val_score(model_KNN,dfx,y,cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test3 = model_rnd_frst.predict(dfx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test3 = np.exp(y_test3) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test3 = pd.DataFrame(y_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test3.shpae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([test['id'], y_test3], axis=1)\n",
    "submission.columns = ['id','trip_duration']\n",
    "submission['trip_duration'] = submission.apply(lambda x : 1 if (x['trip_duration'] <= 0) else x['trip_duration'], axis = 1)\n",
    "submission.to_csv(\"submission_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.stats.probplot(result.resid, plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Regression Plot\n",
    "fig = sm.graphics.plot_regress_exog(result, \"distance\")\n",
    "fig.suptitle(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,20))\n",
    "sm.graphics.plot_partregress_grid(result_boston, fig=fig)\n",
    "fig.suptitle(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이분산성(heteroskedastic) : 독립변수의 값이 커지면 종속변수의 분산도 커지는 것\n",
    "plt.scatter(df[\"x9\"], result.resid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cook's Distance\n",
    "sm.graphics.plot_leverage_resid2(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.influence_plot(result, plot_alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fox recommentation \n",
    "from statsmodels.graphics import utils\n",
    "\n",
    "cooks_d2, pvals = influence.cooks_distance\n",
    "fox_cr = 4 / (len(y) - 2)\n",
    "idx = np.where(cooks_d2 > fox_cr)[0]\n",
    "\n",
    "ax = plt.subplot()\n",
    "plt.scatter(X0, y)\n",
    "plt.scatter(X0[idx], y[idx], s=300, c=\"r\", alpha=0.5)\n",
    "utils.annotate_axes(range(len(idx)), idx, \n",
    "                    list(zip(X0[idx], y[idx])), [(-20, 15)] * len(idx), size=\"small\", ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
